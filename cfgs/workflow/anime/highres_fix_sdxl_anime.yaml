dtype: fp16
amp: fp16
bs: 1
seed: 42

pretrained_model: 'KBlueLeaf/Kohaku-XL-Delta'
prompt: '1girl, (fukuro daizi:1.1), kz oji, (henreader:0.9), (ask \(askzy\), aki99, ama mitsuki, ciloranko:0.9), kaede \(sayappa\), masterpiece, newest, absurdres, solo, dragon girl, dragon tail, dragon wings, dragon horns, white dress, long hair, side up, river, tree, forest, pointy ears, 1girl, :3, angel wings, bell, blonde hair, blue background, blue hair, blush, breasts, collarbone, dress, eyes visible through hair, fang, fire, full body, hair between eyes, horns, looking at viewer, nature, off shoulder, open mouth, orange eyes, outdoors, smile, tail, twintails, wings'
neg_prompt: 'low quality, worst quality, normal quality, text, signature, jpeg artifacts, bad anatomy, old, early, mini skirt, nsfw, chibi'
N_repeats: 3

clip_skip: 0 #动漫模型通常会跳过一个CLIP层
emb_dir: 'embs/'

infer_args:
  init_width: 598
  init_height: 896
  width: 896
  height: 1344  # image size
  guidance_scale: 5  # scale, when higher, the images will tend to be more similar
  num_inference_steps: 20  # how many steps
  hires_inference_steps: 18  # how many steps for highres fix
  scheduler: # DPM++ 2M Karras
    _target_: diffusers.DPMSolverMultistepScheduler
    beta_start: 0.00085
    beta_end: 0.012
    algorithm_type: dpmsolver++
    beta_schedule: scaled_linear
    use_karras_sigmas: true

output_dir: 'output/'

memory: { }

prepare:
  - _target_: hcpdiff.workflow.LoadModelsAction
    pretrained_model: ${pretrained_model}
    dtype: ${dtype}
    scheduler: ${infer_args.scheduler}
  - _target_: hcpdiff.workflow.XformersEnableAction
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      import torch
      from hcpdiff.utils.net_utils import to_cpu, to_cuda
      to_cuda(memory.unet)
      to_cuda(memory.text_encoder)
      memory.vae.to(dtype=torch.bfloat16)
      #to_cuda(memory.vae)
  - _target_: hcpdiff.workflow.PrepareDiffusionAction
    dtype: ${dtype}
    amp: ${amp}
  - _target_: hcpdiff.workflow.VaeOptimizeAction
    slicing: True

actions:
  - _target_: hcpdiff.workflow.TextHookAction # text encoder and tokenizer auto get from memory
    N_repeats: ${N_repeats}
    layer_skip: ${clip_skip}
    emb_dir: ${emb_dir}
    TE_final_norm: false

  # encode text
  - _target_: hcpdiff.workflow.AttnMultTextEncodeAction
    prompt: ${prompt}
    negative_prompt: ${neg_prompt}
    bs: ${bs}
  # prepare seed
  - _target_: hcpdiff.workflow.SeedAction
    seed: ${seed}
  - _target_: hcpdiff.workflow.MakeTimestepsAction
    N_steps: ${infer_args.num_inference_steps}
  # text to image
  - _target_: hcpdiff.workflow.MakeLatentAction
    width: ${infer_args.init_width}
    height: ${infer_args.init_height}
  - _target_: hcpdiff.workflow.LoopAction
    loop_value:
      timesteps: t
    actions:
      - _target_: hcpdiff.workflow.DiffusionStepAction
        guidance_scale: ${infer_args.guidance_scale}

  # image to image
  - _target_: hcpdiff.workflow.LatentResizeAction
  - _target_: hcpdiff.workflow.SeedAction
    seed: ${seed}
  - _target_: hcpdiff.workflow.MakeTimestepsAction
    N_steps: ${infer_args.num_inference_steps}
  # only part of timesteps
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      states['timesteps'] = states['timesteps'][-${infer_args.hires_inference_steps}:]
      states['start_timestep'] = states['timesteps'][:1]
  - _target_: hcpdiff.workflow.MakeLatentAction
    width: ${infer_args.width}
    height: ${infer_args.height}
  - _target_: hcpdiff.workflow.LoopAction
    loop_value:
      timesteps: t
    actions:
      - _target_: hcpdiff.workflow.DiffusionStepAction
        guidance_scale: ${infer_args.guidance_scale}

  # decode to image
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      from hcpdiff.utils.net_utils import to_cpu, to_cuda
      to_cpu(memory.unet)
  - _target_: hcpdiff.workflow.DecodeAction
    vae: ${hcp.from_memory:vae}
    offload: true
  - _target_: hcpdiff.workflow.SaveImageAction
    save_root: ${output_dir}
    image_type: png
